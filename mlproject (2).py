# -*- coding: utf-8 -*-
"""MLProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n4rN9NjrN_mw-ZRp07-H911xRz398U7S
"""

import pandas as pd
import numpy as np
import time
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import KFold, cross_val_predict
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score

file_path = '/content/netflix_titles.csv'
data = pd.read_csv(file_path)

data_info = data.info()
print(data_info)

print(data.head())

# Define target column and features
target_column = 'type'
X = data.drop(columns=[target_column, 'show_id', 'title', 'description'])
y = data[target_column]

# Handle missing values
X['director'].fillna("Unknown", inplace=True)
X['cast'].fillna("Unknown", inplace=True)
X['country'].fillna("Unknown", inplace=True)
X['date_added'].fillna("Unknown", inplace=True)
X['rating'].fillna("Unknown", inplace=True)
X['duration'].fillna("Unknown", inplace=True)

# Encode categorical variables
categorical_cols = X.select_dtypes(include=['object']).columns
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le

numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()
X[numeric_cols] = scaler.fit_transform(X[numeric_cols])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

print(f"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}")

# Initialize the model
model = RandomForestClassifier(random_state=0)

# k-fold cross-validation
k = 5
kfold = KFold(n_splits=k, shuffle=True, random_state=0)

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')

# Output results
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean accuracy: {cv_scores.mean():.4f}")
print(f"Standard deviation: {cv_scores.std():.4f}")

# Logistic Regression
logreg_model = LogisticRegression(random_state=0, max_iter=1000)

# Random Forest
rf_model = RandomForestClassifier(random_state=0)

# Gradient Boosting
gb_model = GradientBoostingClassifier(random_state=0)

models = {
    "Logistic Regression": logreg_model,
    "Random Forest": rf_model,
    "Gradient Boosting": gb_model
}

# K-fold Cross-Validation for others
k = 5
stratified_kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=0)

cv_results = {}

# Evaluation metrics scorers
scorers = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, average='weighted', zero_division=0),
    'recall': make_scorer(recall_score, average='weighted', zero_division=0),
    'f1': make_scorer(f1_score, average='weighted', zero_division=0)
}

# Models Evaluation
for model_name, model in models.items():
    print(f"Evaluating: {model_name}")
    metrics = {
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': []
    }
    for train_idx, test_idx in stratified_kfold.split(X_train, y_train):
        X_fold_train, X_fold_test = X_train.iloc[train_idx], X_train.iloc[test_idx]
        y_fold_train, y_fold_test = y_train.iloc[train_idx], y_train.iloc[test_idx]
        model.fit(X_fold_train, y_fold_train)
        y_pred = model.predict(X_fold_test)
        metrics['accuracy'].append(accuracy_score(y_fold_test, y_pred))
        metrics['precision'].append(precision_score(y_fold_test, y_pred, average='weighted', zero_division=0))
        metrics['recall'].append(recall_score(y_fold_test, y_pred, average='weighted', zero_division=0))
        metrics['f1'].append(f1_score(y_fold_test, y_pred, average='weighted', zero_division=0))

    cv_results[model_name] = {
        'accuracy_mean': np.mean(metrics['accuracy']),
        'accuracy_std': np.std(metrics['accuracy']),
        'precision_mean': np.mean(metrics['precision']),
        'precision_std': np.std(metrics['precision']),
        'recall_mean': np.mean(metrics['recall']),
        'recall_std': np.std(metrics['recall']),
        'f1_mean': np.mean(metrics['f1']),
        'f1_std': np.std(metrics['f1']),
    }

    print(f"Accuracy: {cv_results[model_name]['accuracy_mean']:.4f} ± {cv_results[model_name]['accuracy_std']:.4f}")
    print(f"Precision: {cv_results[model_name]['precision_mean']:.4f} ± {cv_results[model_name]['precision_std']:.4f}")
    print(f"Recall: {cv_results[model_name]['recall_mean']:.4f} ± {cv_results[model_name]['recall_std']:.4f}")
    print(f"F1-Score: {cv_results[model_name]['f1_mean']:.4f} ± {cv_results[model_name]['f1_std']:.4f}")
    print("-" * 40)

# Best Model
best_model_name = max(cv_results, key=lambda x: cv_results[x]['f1_mean'])
print(f"Best model based on F1-Score: {best_model_name}")

# Hypertuning
param_grids = {
    "Logistic Regression": {
        'C': [0.1, 1, 10],
        'solver': ['lbfgs', 'liblinear']
    },
    "Random Forest": {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5]
    },
    "Gradient Boosting": {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7]
    }
}

tuned_results = {}

# Training models with hyperparameter optimization
for model_name, model in models.items():
    print(f"Hyperparameter Tuning for: {model_name}")

    # GridSearchCV
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grids[model_name],
        scoring='f1_weighted',
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0),
        n_jobs=-1
    )


    start_time = time.time()
    grid_search.fit(X_train, y_train)
    end_time = time.time()


    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_
    y_pred = best_model.predict(X_test)


    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)


    tuned_results[model_name] = {
        "best_params": best_params,
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "time_taken": end_time - start_time
    }

    print(f"Best Parameters: {best_params}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"Time Taken: {end_time - start_time:.2f} seconds")
    print("-" * 40)

# Comparison of Performance
print("\nPerformance Comparison Before and After Tuning")
for model_name in models.keys():
    print(f"Model: {model_name}")
    print(f"Before Tuning - F1-Score: {cv_results[model_name]['f1_mean']:.4f}")
    print(f"After Tuning  - F1-Score: {tuned_results[model_name]['f1']:.4f}")
    print("-" * 40)

results_table = []

metrics_table = {
    "Model": list(models.keys()),
    "Accuracy (Before Tuning)": [scores["accuracy_mean"] for scores in cv_results.values()],
    "Precision (Before Tuning)": [scores["precision_mean"] for scores in cv_results.values()],
    "Recall (Before Tuning)": [scores["recall_mean"] for scores in cv_results.values()],
    "F1-Score (Before Tuning)": [scores["f1_mean"] for scores in cv_results.values()],
    "Accuracy (After Tuning)": [tuned_results[model]["accuracy"] for model in models.keys()],
    "Precision (After Tuning)": [tuned_results[model]["precision"] for model in models.keys()],
    "Recall (After Tuning)": [tuned_results[model]["recall"] for model in models.keys()],
    "F1-Score (After Tuning)": [tuned_results[model]["f1"] for model in models.keys()],
}

results_df = pd.DataFrame(metrics_table)

sns.set(style="whitegrid")

before_tuning = [scores["f1_mean"] for scores in cv_results.values()]
after_tuning = [tuned_results[model]["f1"] for model in models.keys()]

x_labels = list(models.keys())

plt.figure(figsize=(10, 6))
x = range(len(models))

plt.bar(x, before_tuning, width=0.4, label="Before Tuning", align='center', color='skyblue')
plt.bar(x, after_tuning, width=0.4, label="After Tuning", align='edge', color='orange')

from tabulate import tabulate

print(tabulate(results_df, headers='keys', tablefmt='grid'))

"""Now, we will do unsupervised learning."""

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# --- 1. KMeans Clustering for Feature Engineering ---
n_clusters = 5
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
X['Cluster'] = kmeans.fit_predict(X)
print("KMeans clustering tamamlandı ve 'Cluster' sütunu eklendi.")

# --- 2. PCA for Dimensionality Reduction ---
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=X['Cluster'], cmap='viridis', s=50)
plt.colorbar(label='Cluster')
plt.title("PCA ile Boyut İndirgeme ve Kümeleme Görselleştirmesi")
plt.xlabel("PCA Bileşeni 1")
plt.ylabel("PCA Bileşeni 2")
plt.show()

X['PCA_1'] = X_pca[:, 0]
X['PCA_2'] = X_pca[:, 1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

model = RandomForestClassifier(random_state=0)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Evaluation Metrics:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

metrics = {
    "Metric": ["Accuracy", "Precision", "Recall", "F1-Score"],
    "Score": [accuracy, precision, recall, f1]
}
metrics_df = pd.DataFrame(metrics)

print("\nPerformance Metrics Table:")
print(metrics_df)